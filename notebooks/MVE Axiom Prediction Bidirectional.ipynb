{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149251b0",
   "metadata": {},
   "source": [
    "# MVE Axiom Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a6ff94",
   "metadata": {},
   "source": [
    "% https://colab.research.google.com/drive/1DIQm9rOx2mT1bZETEeVUThxcrP1RKqAn#scrollTo=IS1dPinuyPCy reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a17f59bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=\"\"\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3924fa30",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09bbeaf",
   "metadata": {},
   "source": [
    "\n",
    "* Try to get a reasonable score?\n",
    "* TODO add batchnorm [done]\n",
    "* TODO add residual connections [done]\n",
    "* TODO add other directions? [done]\n",
    "* TODO add FileWriter [done]\n",
    "* TODO separate itno functions [done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667c466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da403f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f587d695",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398e10c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eholden/.pyenv/versions/3.7.5/lib/python3.7/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Embedding\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import GCNConv, Linear\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_geometric.transforms import to_undirected, ToUndirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "626814a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path(os.path.abspath(\"\")).parent))\n",
    "\n",
    "import config\n",
    "from dataset import get_data_loader, BenchmarkType\n",
    "from stats_writer import Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaf2403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15506878",
   "metadata": {},
   "source": [
    "## CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d81ad412",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ID = '../id_files/train.txt'\n",
    "#TRAIN_ID = \"../id_files/validation.txt\"\n",
    "\n",
    "VAL_ID = \"../id_files/validation.txt\"\n",
    "BENCHMARK_TYPE = BenchmarkType(\"deepmath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b45e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd1d8321570>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1234567)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb7e581",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fb7de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_params = {'transform': ToUndirected()}\n",
    "dataset_params = {\"transform\": None}\n",
    "\n",
    "# transform = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f39d8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: TorchMemoryDataset(22179)\n"
     ]
    }
   ],
   "source": [
    "data_instance = get_data_loader(TRAIN_ID, BENCHMARK_TYPE, **dataset_params)\n",
    "#val_data = get_data_loader(VAL_ID, BENCHMARK_TYPE, **dataset_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9679046",
   "metadata": {},
   "source": [
    "## Data point check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1af35668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[46632], edge_index=[2, 77783], premise_index=[968], conjecture_index=[64], name=[64], y=[968], batch=[46632], ptr=[65])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "724ecb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[982], edge_index=[2, 1771], premise_index=[14], conjecture_index=[1], name='t29_bvfunc11', y=[14])\n",
      "['conjecture_index', 'x', 'edge_index', 'premise_index', 'y', 'name']\n",
      "982\n",
      "1771\n",
      "1\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(data_instance))[0]\n",
    "print(data)\n",
    "print(data.keys)\n",
    "print(data.num_nodes)\n",
    "print(data.num_edges)\n",
    "print(data.num_node_features)\n",
    "print(data.has_isolated_nodes())\n",
    "print(data.is_directed())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e050678e",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2316008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_output_network(no_dense_layers, hidden_dim, task, dropout_rate):\n",
    "    if task == \"premise\":\n",
    "        return DensePremiseOutput(no_dense_layers, hidden_dim, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4338d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e919359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensePremiseOutput(torch.nn.Module):\n",
    "    def __init__(self, no_dense_layers, hidden_dim, dropout_rate):\n",
    "        super(DensePremiseOutput, self).__init__()\n",
    "\n",
    "        self.no_dense_layers = no_dense_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.lin = nn.ModuleList()\n",
    "        for _ in range(no_dense_layers):\n",
    "            self.lin.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "\n",
    "        # Add output layer\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, premise_index):\n",
    "\n",
    "        # Extract premises\n",
    "        x = x[premise_index]\n",
    "\n",
    "        # Dense feedforward\n",
    "        for i in range(self.no_dense_layers):\n",
    "            x = self.lin[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.out(x)\n",
    "        x = x.squeeze(-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd8714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84a57fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bda9bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6034f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCN_NORMALISATION = {\"batch\": nn.BatchNorm1d, \"layer\": nn.LayerNorm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e1749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8838a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_normalisation_layers(normaliser, hidden_dim, num_normalisation_layers):\n",
    "\n",
    "    lns = nn.ModuleList()\n",
    "    for _ in range(num_normalisation_layers):\n",
    "        lns.append(normaliser(hidden_dim))\n",
    "    return lns\n",
    "\n",
    "\n",
    "def build_conv_model(num_convolutional_layers, hidden_dim, flow):\n",
    "\n",
    "    convs = nn.ModuleList()\n",
    "    for _ in range(num_convolutional_layers):\n",
    "        convs.append(get_conv_layer(hidden_dim, hidden_dim, flow))\n",
    "\n",
    "    return convs\n",
    "\n",
    "\n",
    "def get_conv_layer(input_dim, hidden_dim, flow):\n",
    "    return pyg_nn.GCNConv(input_dim, hidden_dim, flow=flow)\n",
    "\n",
    "\n",
    "def build_merge_linear_layers(num_linear_layers, hidden_dim):\n",
    "\n",
    "    lin = nn.ModuleList()\n",
    "    for _ in range(num_linear_layers):\n",
    "        lin.append(Linear(hidden_dim * 2, hidden_dim))\n",
    "\n",
    "    return lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d218899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNDirectional(\n",
       "  (convs): ModuleList(\n",
       "    (0): GCNConv(32, 32)\n",
       "    (1): GCNConv(32, 32)\n",
       "    (2): GCNConv(32, 32)\n",
       "  )\n",
       "  (lns): ModuleList(\n",
       "    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GCNDirectional(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_convolutional_layers, dropout_rate, normalisation, skip_connection):\n",
    "        super(GCNDirectional, self).__init__()\n",
    "\n",
    "        self.flow = \"target_to_source\"  # Sets direction to bottom up\n",
    "\n",
    "        # Set variables\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_convolutional_layers = num_convolutional_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.skip_connection = skip_connection\n",
    "\n",
    "        # Add convolutional layers\n",
    "        self.convs = build_conv_model(num_convolutional_layers, hidden_dim, self.flow)\n",
    "\n",
    "        # Add normalisation layers used in between graph convolutions\n",
    "        if normalisation is None:\n",
    "            self.lns = None\n",
    "        else:\n",
    "            self.normaliser = GCN_NORMALISATION[normalisation]\n",
    "            self.lns = build_normalisation_layers(self.normaliser, hidden_dim, num_convolutional_layers - 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        # Iterate over each convolutional sequence\n",
    "        for i in range(self.num_convolutional_layers):\n",
    "\n",
    "            conv_out = self.convs[i](x, edge_index)\n",
    "            # Check if applying skip connection\n",
    "            if self.skip_connection:\n",
    "                x = x + conv_out\n",
    "            else:\n",
    "                x = conv_out\n",
    "\n",
    "            emb = x\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "            if self.lns is not None and not i == self.num_convolutional_layers - 1:  # Apply normalisation\n",
    "                x = self.lns[i](x)\n",
    "\n",
    "        return emb, x\n",
    "\n",
    "\n",
    "sub = GCNDirectional(\n",
    "    hidden_dim=32, num_convolutional_layers=3, dropout_rate=0.25, normalisation=\"batch\", skip_connection=False\n",
    ")\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43775f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16bbf39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dbc42b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b5bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cc71979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNBiDirectional(\n",
       "  (convs_up): ModuleList(\n",
       "    (0): GCNConv(32, 32)\n",
       "    (1): GCNConv(32, 32)\n",
       "    (2): GCNConv(32, 32)\n",
       "  )\n",
       "  (convs_down): ModuleList(\n",
       "    (0): GCNConv(32, 32)\n",
       "    (1): GCNConv(32, 32)\n",
       "    (2): GCNConv(32, 32)\n",
       "  )\n",
       "  (lns): ModuleList(\n",
       "    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (linear): ModuleList(\n",
       "    (0): Linear(64, 32, bias=True)\n",
       "    (1): Linear(64, 32, bias=True)\n",
       "    (2): Linear(64, 32, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GCNBiDirectional(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_convolutional_layers, dropout_rate, normalisation, skip_connection):\n",
    "        super(GCNBiDirectional, self).__init__()\n",
    "\n",
    "        # Set variables\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_convolutional_layers = num_convolutional_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.skip_connection = skip_connection\n",
    "\n",
    "        # Add convolutional layers\n",
    "        self.convs_up = build_conv_model(num_convolutional_layers, hidden_dim, flow=\"source_to_target\")\n",
    "        self.convs_down = build_conv_model(num_convolutional_layers, hidden_dim, flow=\"target_to_source\")\n",
    "\n",
    "        # Add normalisation layers used in between graph convolutions\n",
    "        if normalisation is None:\n",
    "            self.lns = None\n",
    "        else:\n",
    "            self.normaliser = GCN_NORMALISATION[normalisation]\n",
    "            self.lns = build_normalisation_layers(self.normaliser, hidden_dim, num_convolutional_layers - 1)\n",
    "\n",
    "        # Add Linear layers\n",
    "        self.linear = build_merge_linear_layers(num_convolutional_layers, hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        # Iterate over each convolutional sequence\n",
    "        for i in range(self.num_convolutional_layers):\n",
    "\n",
    "            # Apply convolutions\n",
    "            x_up = self.convs_up[i](x, edge_index)\n",
    "            x_down = self.convs_down[i](x, edge_index)\n",
    "\n",
    "            # Check if applying skip connection\n",
    "            if self.skip_connection:\n",
    "                x_up = x + x_up\n",
    "                x_down = x + x_down\n",
    "\n",
    "            # Concat convolutions\n",
    "            x = torch.cat((x_up, x_down), dim=1)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "            # Merge through linear\n",
    "            x = self.linear[i](x)\n",
    "            emb = x\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "            # Normalise, if set\n",
    "            if self.lns is not None and not i == self.num_convolutional_layers - 1:  # Apply normalisation\n",
    "                x = self.lns[i](x)\n",
    "\n",
    "        return emb, x\n",
    "\n",
    "\n",
    "sub = GCNBiDirectional(\n",
    "    hidden_dim=32, num_convolutional_layers=3, dropout_rate=0.25, normalisation=\"batch\", skip_connection=False\n",
    ")\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb21e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854bd26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8021b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNStack(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim,\n",
    "        num_convolutional_layers,\n",
    "        no_dense_layers,\n",
    "        direction,\n",
    "        dropout_rate=0.0,\n",
    "        task=\"premise\",\n",
    "        normalisation=\"layer\",\n",
    "        skip_connection=True,\n",
    "    ):\n",
    "        super(GNNStack, self).__init__()\n",
    "\n",
    "        # Set variables\n",
    "        self.task = task\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Add embedding layer\n",
    "        self.node_embedding = Embedding(len(config.NODE_TYPE), hidden_dim)\n",
    "\n",
    "        # Add GCN layer\n",
    "        if direction == \"single\":\n",
    "            gcn_base = GCNDirectional\n",
    "        elif direction == \"separate\":\n",
    "            gcn_base = GCNBiDirectional\n",
    "        else:\n",
    "            raise ValueError(f\"Unkown gcn direction {direction}\")\n",
    "\n",
    "        self.gcn = gcn_base(\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_convolutional_layers=num_convolutional_layers,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            normalisation=normalisation,\n",
    "            skip_connection=skip_connection,\n",
    "        )\n",
    "\n",
    "        # Post-message-passing\n",
    "        self.post_mp = get_dense_output_network(\n",
    "            no_dense_layers, hidden_dim, task=self.task, dropout_rate=self.dropout_rate\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, premise_index = data.x, data.edge_index, data.premise_index\n",
    "\n",
    "        x = self.node_embedding(x)\n",
    "\n",
    "        emb, x = self.gcn(x, edge_index)\n",
    "\n",
    "        if self.task == \"premise\":\n",
    "            x = self.post_mp(x, premise_index)\n",
    "\n",
    "            # elif self.task == 'graph':\n",
    "            #    x = pyg_nn.global_mean_pool(x, batch)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        return emb, x\n",
    "\n",
    "    #def loss(self, pred, label):\n",
    "    #    return F.nll_loss(pred, label)\n",
    "\n",
    "\n",
    "test_model = GNNStack(\n",
    "    hidden_dim=32,\n",
    "    num_convolutional_layers=3,\n",
    "    no_dense_layers=1,\n",
    "    direction=\"single\",\n",
    "    dropout_rate=0.25,\n",
    "    task=\"premise\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b98b826e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNStack(\n",
      "  (node_embedding): Embedding(15, 32)\n",
      "  (gcn): GCNDirectional(\n",
      "    (convs): ModuleList(\n",
      "      (0): GCNConv(32, 32)\n",
      "      (1): GCNConv(32, 32)\n",
      "      (2): GCNConv(32, 32)\n",
      "    )\n",
      "    (lns): ModuleList(\n",
      "      (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (post_mp): DensePremiseOutput(\n",
      "    (lin): ModuleList(\n",
      "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "    )\n",
      "    (out): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee52ea03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b3740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2985c979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82dfdfcb",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081eb353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69431792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, criterion, optimizer):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for data in train_data:  # Iterate in batches over the training dataset.\n",
    "        _, out = model(data)  # Perform a single forward pass. TODO change this\n",
    "\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        \n",
    "\n",
    "def test(model, test_data, writer, testing: bool):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    for data in test_data:  # Iterate in batches over the training/test dataset.\n",
    "        _, out = model(data)\n",
    "        pred = torch.sigmoid(out).round().long()\n",
    "\n",
    "        correct += data.y.eq(pred).sum().item()\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(out, data.y, reduction='sum')\n",
    "        total_loss += loss\n",
    "        total_samples += len(pred)\n",
    "        \n",
    "    acc_score = correct / total_samples # Derive ratio of correct predictions.\n",
    "    total_loss /= total_samples\n",
    "    total_loss = total_loss.item()\n",
    "\n",
    "    \n",
    "    if testing:\n",
    "        writer.report_val_score(acc_score)\n",
    "        writer.report_val_loss(total_loss)\n",
    "    else:\n",
    "        writer.report_train_score(acc_score)\n",
    "        writer.report_train_loss(total_loss)\n",
    "        \n",
    "    return  acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fa6e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714b9721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf5dca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: TorchMemoryDataset(22179)\n",
      "Dataset: TorchMemoryDataset(2465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 16:29:56.293760: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training\n"
     ]
    }
   ],
   "source": [
    "def train_loop():\n",
    "    \n",
    "    train_data = get_data_loader(TRAIN_ID, BENCHMARK_TYPE, **dataset_params)\n",
    "    val_data = get_data_loader(VAL_ID, BENCHMARK_TYPE, **dataset_params)\n",
    "\n",
    "    model = GNNStack(\n",
    "            hidden_dim=16,\n",
    "            num_convolutional_layers=1,\n",
    "            no_dense_layers=1,\n",
    "            direction=\"single\",\n",
    "            dropout_rate=0.2,\n",
    "            task=\"premise\",\n",
    "    )\n",
    "    writer = Writer(model)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "    #criterion = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "\n",
    "    print('\\nTraining')\n",
    "    for epoch in range(1, 5):\n",
    "\n",
    "\n",
    "        train(model, train_data, criterion, optimizer)\n",
    "\n",
    "        train_acc = test(model, train_data, writer, testing=False)\n",
    "        test_acc = test(model, val_data, writer, testing=True)\n",
    "        print(f\"Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "        writer.on_step()\n",
    "\n",
    "\n",
    "    return writer\n",
    "writer = train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16656d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395f46f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.get_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ccbcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725e64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9f05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5262a633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e92eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.node_embedding = Embedding(len(config.NODE_TYPE), hidden_channels)\n",
    "\n",
    "\n",
    "        self.conv1 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        self.linear = Linear(hidden_channels, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        \n",
    "        x = input_batch.x\n",
    "        edge_index = input_batch.edge_index\n",
    "        premise_index = input_batch.premise_index    \n",
    "\n",
    "        x = self.node_embedding(x)\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = F.dropout(x, p=0.5, training=self.training) # TODO add dropout parameter?\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = x[premise_index]\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # Remove inner axis\n",
    "        x = x.squeeze(-1)\n",
    "        \n",
    "    \n",
    "        return x\n",
    "\n",
    "    \n",
    "model = GCN(hidden_channels=16)\n",
    "print(model)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
